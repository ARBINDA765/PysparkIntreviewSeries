{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM3uAJH4roCCz3uPE4qK9/N",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ARBINDA765/PysparkIntreviewSeries/blob/main/Json_Fatten.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "id": "M9gnOshYepnu",
        "outputId": "a1649c4d-9e2f-4e01-884b-e0f601fcee20"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7970be7147c0>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://890345db6195:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.5.1</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>pyspark-shell</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q http://archive.apache.org/dist/spark/spark-3.5.1/spark-3.5.1-bin-hadoop3.tgz\n",
        "!tar xf spark-3.5.1-bin-hadoop3.tgz\n",
        "!pip install -q findspark\n",
        "import findspark\n",
        "\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.5.1-bin-hadoop3\"\n",
        "\n",
        "findspark.init()\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
        "spark.conf.set(\"spark.sql.repl.eagerEval.enabled\", True) # Property used to format output tables better\n",
        "spark"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "HSjDy4UcfyhR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df=spark.read.format(\"json\").option(\"multiline\", \"true\").load(\"/content/multiline-zipcode.json\")\n",
        "df.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zKUHJqHsesHA",
        "outputId": "ff102489-43ca-4937-997e-cfa87ebb6517"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- City: string (nullable = true)\n",
            " |-- RecordNumber: long (nullable = true)\n",
            " |-- State: string (nullable = true)\n",
            " |-- ZipCodeType: string (nullable = true)\n",
            " |-- Zipcode: long (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.write.partitionBy('City').mode(\"append\").format(\"json\").option(\"compression\", \"snappy\").option(\"dateFormat\", \"yyyy-MM-dd\").option(\"linesep\",\",\").save(\"/content/output\")\n",
        "#.option(\"dateFormat\", \"yyyy-MM-dd\").option(\"timestampFormat\", \"yyyy-MM-dd'T'HH:mm:ss.SSSXXX\").option(\"lineSep\", \"\\n\")."
      ],
      "metadata": {
        "id": "E1S8dT_5fsO9"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls /content/output"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y3fpSkL-kiOv",
        "outputId": "bab04c89-fa79-44a3-d79e-0a52264f8a3f"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'City=BDA SAN LUIS'  'City=PASEO COSTA DEL SUR'   _SUCCESS\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "json_snappy=spark.read.format(\"json\").load(\"/content/output/City=BDA SAN LUIS/part-00000-c64d543f-08ec-41d5-a53a-e73abad907cc.c000.json.snappy\")"
      ],
      "metadata": {
        "id": "8xqckU5ni--a"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "json_snappy.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r1k8ss4hjFLw",
        "outputId": "67690ad3-f3e6-45b4-fe4b-fb1ccd85f8c8"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------+-----+-----------+-------+\n",
            "|RecordNumber|State|ZipCodeType|Zipcode|\n",
            "+------------+-----+-----------+-------+\n",
            "|          10|   PR|   STANDARD|    709|\n",
            "+------------+-----+-----------+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import *\n",
        "from pyspark.sql.types import *\n",
        "explode_df=spark.read.format(\"json\").option(\"multiline\",\"true\").load(\"/content/multiline json.json\")"
      ],
      "metadata": {
        "id": "X-EyhS1bkNYC"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "explode_df.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jWuf2L4To_Oe",
        "outputId": "c41b03a3-a6a5-4f5e-e57f-94ee047f6771"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- customer_id: string (nullable = true)\n",
            " |-- name: string (nullable = true)\n",
            " |-- orders: array (nullable = true)\n",
            " |    |-- element: struct (containsNull = true)\n",
            " |    |    |-- order_id: long (nullable = true)\n",
            " |    |    |-- product: string (nullable = true)\n",
            " |    |    |-- quantity: long (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "flattern_df=explode_df.select(\"customer_id\",\"name\",explode(\"orders\").alias(\"order\"))\n",
        "flattern_df=flattern_df.select(\"customer_id\",\"name\",\"order.order_id\",\"order.product\",\"order.quantity\")\n",
        "flattern_df.show()\n",
        "flattern_df.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z5p1pgd0pB2b",
        "outputId": "83d92bc6-5c15-4aae-fb58-a378168d1017"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+----------+--------+----------+--------+\n",
            "|customer_id|      name|order_id|   product|quantity|\n",
            "+-----------+----------+--------+----------+--------+\n",
            "|        001|  John Doe|     101|    Laptop|       2|\n",
            "|        001|  John Doe|     102|     Phone|       1|\n",
            "|        002|Jane Smith|     103|    Tablet|       3|\n",
            "|        002|Jane Smith|     104|Headphones|       2|\n",
            "+-----------+----------+--------+----------+--------+\n",
            "\n",
            "root\n",
            " |-- customer_id: string (nullable = true)\n",
            " |-- name: string (nullable = true)\n",
            " |-- order_id: long (nullable = true)\n",
            " |-- product: string (nullable = true)\n",
            " |-- quantity: long (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_2=spark.read.format(\"json\").option(\"multiline\",\"true\").load(\"/content/Nested_json_flattered.json\")\n",
        "df_2.printSchema()\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark.sql.types import *\n",
        "df_3=df_2.withColumn(\"city\",df_2.address.city) \\\n",
        ".withColumn(\"state\",df_2.address.state) \\\n",
        ".drop(\"address\") \\\n",
        ".withColumn(\"order\",explode(\"orders\")).select(\"*\",\"order.*\").drop(\"orders\",\"order\")\n",
        "\n",
        "df_3.printSchema()\n",
        "df_3.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FYBEOETpp2rM",
        "outputId": "91d020da-0df6-47e1-f579-9888925f904e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- address: struct (nullable = true)\n",
            " |    |-- city: string (nullable = true)\n",
            " |    |-- state: string (nullable = true)\n",
            " |-- age: long (nullable = true)\n",
            " |-- id: long (nullable = true)\n",
            " |-- name: string (nullable = true)\n",
            " |-- orders: array (nullable = true)\n",
            " |    |-- element: struct (containsNull = true)\n",
            " |    |    |-- order_id: long (nullable = true)\n",
            " |    |    |-- productname: string (nullable = true)\n",
            "\n",
            "root\n",
            " |-- age: long (nullable = true)\n",
            " |-- id: long (nullable = true)\n",
            " |-- name: string (nullable = true)\n",
            " |-- city: string (nullable = true)\n",
            " |-- state: string (nullable = true)\n",
            " |-- order_id: long (nullable = true)\n",
            " |-- productname: string (nullable = true)\n",
            "\n",
            "+---+---+-----+-------------+-----+--------+-----------+\n",
            "|age| id| name|         city|state|order_id|productname|\n",
            "+---+---+-----+-------------+-----+--------+-----------+\n",
            "| 30|  1| John|     New York|   NY|     101|     Laptop|\n",
            "| 30|  1| John|     New York|   NY|     102| Headphones|\n",
            "| 25|  2|Alice|San Francisco|   CA|     103| Smartphone|\n",
            "+---+---+-----+-------------+-----+--------+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "source": [
        "from pyspark.sql import DataFrame\n",
        "from pyspark.sql.functions import col, explode_outer\n",
        "from pyspark.sql.types import ArrayType, StructType\n",
        "\n",
        "def flatten_json(df: DataFrame) -> DataFrame:\n",
        "    \"\"\"\n",
        "    Flattens a DataFrame with complex nested fields (Arrays and Structs) by converting them into individual columns.\n",
        "\n",
        "    Parameters:\n",
        "    - df: The input DataFrame with complex nested fields\n",
        "\n",
        "    Returns:\n",
        "    - The flattened DataFrame with all complex fields expanded into separate columns.\n",
        "    \"\"\"\n",
        "    # Compute Complex Fields (Lists and Structs) in Schema\n",
        "    complex_fields = dict([(field.name, field.dataType)\n",
        "                           for field in df.schema.fields\n",
        "                           if isinstance(field.dataType, ArrayType) or isinstance(field.dataType, StructType)])\n",
        "\n",
        "    print(df.schema)\n",
        "    print(\"\")\n",
        "\n",
        "    while len(complex_fields) != 0:\n",
        "        col_name = list(complex_fields.keys())[0]\n",
        "        print(\"Processing :\" + col_name + \" Type : \" + str(type(complex_fields[col_name])))\n",
        "\n",
        "        # If StructType then convert all sub elements to columns.\n",
        "        # i.e. flatten structs\n",
        "        if isinstance(complex_fields[col_name], StructType):\n",
        "            expanded = [col(col_name + '.' + k).alias(col_name + '_' + k) for k in [n.name for n in complex_fields[col_name]]]\n",
        "            df = df.select(\"*\", *expanded).drop(col_name)\n",
        "\n",
        "        # If ArrayType then add the Array Elements as Rows using the explode function\n",
        "        # i.e. explode Arrays\n",
        "        elif isinstance(complex_fields[col_name], ArrayType):\n",
        "            df = df.withColumn(col_name, explode_outer(col_name))\n",
        "\n",
        "        # Recompute remaining Complex Fields in Schema\n",
        "        complex_fields = dict([(field.name, field.dataType)\n",
        "                               for field in df.schema.fields\n",
        "                               if isinstance(field.dataType, ArrayType) or isinstance(field.dataType, StructType)])\n",
        "\n",
        "    return df\n"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "cXjwmDVo0Hp-"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "flatten_json(df_2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 217
        },
        "id": "wV9yw6lX0Iqe",
        "outputId": "7738c115-fae8-467d-f423-b96e84c56d20"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "StructType([StructField('address', StructType([StructField('city', StringType(), True), StructField('state', StringType(), True)]), True), StructField('age', LongType(), True), StructField('id', LongType(), True), StructField('name', StringType(), True), StructField('orders', ArrayType(StructType([StructField('order_id', LongType(), True), StructField('productname', StringType(), True)]), True), True)])\n",
            "\n",
            "Processing :address Type : <class 'pyspark.sql.types.StructType'>\n",
            "Processing :orders Type : <class 'pyspark.sql.types.ArrayType'>\n",
            "Processing :orders Type : <class 'pyspark.sql.types.StructType'>\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "+---+---+-----+-------------+-------------+---------------+------------------+\n",
              "|age| id| name| address_city|address_state|orders_order_id|orders_productname|\n",
              "+---+---+-----+-------------+-------------+---------------+------------------+\n",
              "| 30|  1| John|     New York|           NY|            101|            Laptop|\n",
              "| 30|  1| John|     New York|           NY|            102|        Headphones|\n",
              "| 25|  2|Alice|San Francisco|           CA|            103|        Smartphone|\n",
              "+---+---+-----+-------------+-------------+---------------+------------------+"
            ],
            "text/html": [
              "<table border='1'>\n",
              "<tr><th>age</th><th>id</th><th>name</th><th>address_city</th><th>address_state</th><th>orders_order_id</th><th>orders_productname</th></tr>\n",
              "<tr><td>30</td><td>1</td><td>John</td><td>New York</td><td>NY</td><td>101</td><td>Laptop</td></tr>\n",
              "<tr><td>30</td><td>1</td><td>John</td><td>New York</td><td>NY</td><td>102</td><td>Headphones</td></tr>\n",
              "<tr><td>25</td><td>2</td><td>Alice</td><td>San Francisco</td><td>CA</td><td>103</td><td>Smartphone</td></tr>\n",
              "</table>\n"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Dynamic code for nested json to flat**"
      ],
      "metadata": {
        "id": "9G6iKYse4X_p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.types import *\n",
        "from pyspark.sql.functions import *\n",
        "\n",
        "#Flatten array of structs and structs\n",
        "def flatten(df):\n",
        "   # compute Complex Fields (Lists and Structs) in Schema\n",
        "   complex_fields = dict([(field.name, field.dataType)\n",
        "                             for field in df.schema.fields\n",
        "                             if type(field.dataType) == ArrayType or  type(field.dataType) == StructType])\n",
        "   print(complex_fields)\n",
        "\n",
        "   while len(complex_fields)!=0:\n",
        "      col_name=list(complex_fields.keys())[0]\n",
        "      print (\"Processing :\"+col_name+\" Type : \"+str(type(complex_fields[col_name])))\n",
        "\n",
        "      # if StructType then convert all sub element to columns.\n",
        "      # i.e. flatten structs\n",
        "      if (type(complex_fields[col_name]) == StructType):\n",
        "         expanded = [col(col_name+'.'+k).alias(col_name+'_'+k) for k in [ n.name for n in  complex_fields[col_name]]]\n",
        "         df=df.select(\"*\", *expanded).drop(col_name)\n",
        "\n",
        "      # if ArrayType then add the Array Elements as Rows using the explode function\n",
        "      # i.e. explode Arrays\n",
        "      elif (type(complex_fields[col_name]) == ArrayType):\n",
        "         df=df.withColumn(col_name,explode_outer(col_name))\n",
        "\n",
        "      # recompute remaining Complex Fields in Schema\n",
        "      complex_fields = dict([(field.name, field.dataType)\n",
        "                             for field in df.schema.fields\n",
        "                             if type(field.dataType) == ArrayType or  type(field.dataType) == StructType])\n",
        "      print(complex_fields)\n",
        "   return df\n",
        "\n",
        "flatten(df_2)\n",
        "df_2.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "54AOcHZr0nTB",
        "outputId": "d9e5924f-4d7e-48ca-d154-f8d7186dcf00"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'address': StructType([StructField('city', StringType(), True), StructField('state', StringType(), True)]), 'orders': ArrayType(StructType([StructField('order_id', LongType(), True), StructField('productname', StringType(), True)]), True)}\n",
            "Processing :address Type : <class 'pyspark.sql.types.StructType'>\n",
            "{'orders': ArrayType(StructType([StructField('order_id', LongType(), True), StructField('productname', StringType(), True)]), True)}\n",
            "Processing :orders Type : <class 'pyspark.sql.types.ArrayType'>\n",
            "{'orders': StructType([StructField('order_id', LongType(), True), StructField('productname', StringType(), True)])}\n",
            "Processing :orders Type : <class 'pyspark.sql.types.StructType'>\n",
            "{}\n",
            "root\n",
            " |-- address: struct (nullable = true)\n",
            " |    |-- city: string (nullable = true)\n",
            " |    |-- state: string (nullable = true)\n",
            " |-- age: long (nullable = true)\n",
            " |-- id: long (nullable = true)\n",
            " |-- name: string (nullable = true)\n",
            " |-- orders: array (nullable = true)\n",
            " |    |-- element: struct (containsNull = true)\n",
            " |    |    |-- order_id: long (nullable = true)\n",
            " |    |    |-- productname: string (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import DataFrame\n",
        "from pyspark.sql.functions import col, explode_outer\n",
        "\n",
        "def flatten_1(df: DataFrame) -> DataFrame:\n",
        "    # Function to recursively flatten a DataFrame with complex types\n",
        "    def flatten_struct(df: DataFrame, col_name: str) -> DataFrame:\n",
        "        \"\"\"Flatten a StructType column.\"\"\"\n",
        "        struct_fields = [col(col_name + '.' + field.name).alias(col_name + '_' + field.name) for field in df.schema[col_name].dataType]\n",
        "        return df.select(\"*\", *struct_fields).drop(col_name)\n",
        "\n",
        "    def explode_array(df: DataFrame, col_name: str) -> DataFrame:\n",
        "        \"\"\"Explode an ArrayType column.\"\"\"\n",
        "        return df.withColumn(col_name, explode_outer(col_name))\n",
        "\n",
        "    # Process complex fields (StructType and ArrayType)\n",
        "    complex_fields = {field.name: field.dataType for field in df.schema.fields if isinstance(field.dataType, (ArrayType, StructType))}\n",
        "\n",
        "    while complex_fields:\n",
        "        col_name, data_type = next(iter(complex_fields.items()))\n",
        "\n",
        "        if isinstance(data_type, StructType):\n",
        "            df = flatten_struct(df, col_name)\n",
        "        elif isinstance(data_type, ArrayType):\n",
        "            df = explode_array(df, col_name)\n",
        "\n",
        "        # Update complex fields after modifications\n",
        "        complex_fields = {field.name: field.dataType for field in df.schema.fields if isinstance(field.dataType, (ArrayType, StructType))}\n",
        "\n",
        "    return df\n"
      ],
      "metadata": {
        "id": "bR8zPmPo1B5c"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lACsxWiCpW61"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "flatten_1(df_2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 110
        },
        "id": "w3hqLp8K6OIo",
        "outputId": "bc83f889-c634-4135-a16d-ace1af16a81e"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "+---+---+-----+-------------+-------------+---------------+------------------+\n",
              "|age| id| name| address_city|address_state|orders_order_id|orders_productname|\n",
              "+---+---+-----+-------------+-------------+---------------+------------------+\n",
              "| 30|  1| John|     New York|           NY|            101|            Laptop|\n",
              "| 30|  1| John|     New York|           NY|            102|        Headphones|\n",
              "| 25|  2|Alice|San Francisco|           CA|            103|        Smartphone|\n",
              "+---+---+-----+-------------+-------------+---------------+------------------+"
            ],
            "text/html": [
              "<table border='1'>\n",
              "<tr><th>age</th><th>id</th><th>name</th><th>address_city</th><th>address_state</th><th>orders_order_id</th><th>orders_productname</th></tr>\n",
              "<tr><td>30</td><td>1</td><td>John</td><td>New York</td><td>NY</td><td>101</td><td>Laptop</td></tr>\n",
              "<tr><td>30</td><td>1</td><td>John</td><td>New York</td><td>NY</td><td>102</td><td>Headphones</td></tr>\n",
              "<tr><td>25</td><td>2</td><td>Alice</td><td>San Francisco</td><td>CA</td><td>103</td><td>Smartphone</td></tr>\n",
              "</table>\n"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "df_int=spark.read.format(\"json\").option(\"multiline\",\"true\").load(\"/content/flatten json in pyspark.json\")\n",
        "df_int.printSchema()\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark.sql.types import *\n",
        "df_int=df_int.withColumn(\"branche\",explode(\"branches\")).drop(\"branches\")\n",
        "df_int=df_int.select(\"*\",\"branche.*\").drop(\"branche\")\n",
        "df_int.printSchema()\n",
        "df_int.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ae6thMAU6R_7",
        "outputId": "4ab37123-ce1d-4364-eb81-eb423327b2ec"
      },
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- Course_type: string (nullable = true)\n",
            " |-- Head_Office_Contact: long (nullable = true)\n",
            " |-- Institute_Name: string (nullable = true)\n",
            " |-- branches: array (nullable = true)\n",
            " |    |-- element: struct (containsNull = true)\n",
            " |    |    |-- City: string (nullable = true)\n",
            " |    |    |-- State: string (nullable = true)\n",
            " |    |    |-- address: string (nullable = true)\n",
            "\n",
            "root\n",
            " |-- Course_type: string (nullable = true)\n",
            " |-- Head_Office_Contact: long (nullable = true)\n",
            " |-- Institute_Name: string (nullable = true)\n",
            " |-- City: string (nullable = true)\n",
            " |-- State: string (nullable = true)\n",
            " |-- address: string (nullable = true)\n",
            "\n",
            "+-----------+-------------------+-------------------+------+-----------+-------+\n",
            "|Course_type|Head_Office_Contact|     Institute_Name|  City|      State|address|\n",
            "+-----------+-------------------+-------------------+------+-----------+-------+\n",
            "|Best_seller|         8787878787|ABC_Coaching_Center|Mumbai|Maharashtra|    XYZ|\n",
            "|Best_seller|         8787878787|ABC_Coaching_Center| Surat|     Gujrat|   PQRX|\n",
            "+-----------+-------------------+-------------------+------+-----------+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_int=df_int.withColumn(\"branche\",explode(\"branches\")).drop(\"branches\")\n",
        "df_int=df_int.select(\"*\",\"branche.*\").drop(\"branche\")\n",
        "df_int.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 651
        },
        "id": "UnAfDoUw8B1T",
        "outputId": "fbc83edb-b26e-4b86-cda1-06acae87838c"
      },
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AnalysisException",
          "evalue": "[UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `branches` cannot be resolved. Did you mean one of the following? [`State`, `address`, `City`, `Course_type`, `Institute_Name`].;\n'Project [Course_type#2285, Head_Office_Contact#2286L, Institute_Name#2287, City#2523, State#2524, address#2525, explode('branches) AS branche#2569]\n+- Project [Course_type#2285, Head_Office_Contact#2286L, Institute_Name#2287, City#2523, State#2524, address#2525]\n   +- Project [Course_type#2285, Head_Office_Contact#2286L, Institute_Name#2287, branche#2513, branche#2513.City AS City#2523, branche#2513.State AS State#2524, branche#2513.address AS address#2525]\n      +- Project [Course_type#2285, Head_Office_Contact#2286L, Institute_Name#2287, branche#2513]\n         +- Project [Course_type#2285, Head_Office_Contact#2286L, Institute_Name#2287, branches#2288, branche#2513]\n            +- Generate explode(branches#2288), false, [branche#2513]\n               +- Project [Course_type#2285, Head_Office_Contact#2286L, Institute_Name#2287, branches#2288, branche#2481]\n                  +- Generate explode(branches#2288), false, [branche#2481]\n                     +- Project [Course_type#2285, Head_Office_Contact#2286L, Institute_Name#2287, branches#2288, branche#2449]\n                        +- Generate explode(branches#2288), true, [branche#2449]\n                           +- Project [Course_type#2285, Head_Office_Contact#2286L, Institute_Name#2287, branches#2288, branche#2417]\n                              +- Generate explode(branches#2288), false, [branche#2417]\n                                 +- Project [Course_type#2285, Head_Office_Contact#2286L, Institute_Name#2287, branches#2288, branche#2385]\n                                    +- Generate explode(branches#2288), true, [branche#2385]\n                                       +- Project [Course_type#2285, Head_Office_Contact#2286L, Institute_Name#2287, branches#2288, branche#2378]\n                                          +- Generate explode(branches#2288), true, [branche#2378]\n                                             +- Relation [Course_type#2285,Head_Office_Contact#2286L,Institute_Name#2287,branches#2288] json\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-93-51071579abeb>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_int\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdf_int\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"branche\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mexplode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"branches\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"branches\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdf_int\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdf_int\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"*\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"branche.*\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"branche\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdf_int\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.5.1-bin-hadoop3/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mwithColumn\u001b[0;34m(self, colName, col)\u001b[0m\n\u001b[1;32m   5172\u001b[0m                 \u001b[0mmessage_parameters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"arg_name\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"col\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"arg_type\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5173\u001b[0m             )\n\u001b[0;32m-> 5174\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolName\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparkSession\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5175\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5176\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwithColumnRenamed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexisting\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;34m\"DataFrame\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.5.1-bin-hadoop3/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.5.1-bin-hadoop3/python/pyspark/errors/exceptions/captured.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAnalysisException\u001b[0m: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `branches` cannot be resolved. Did you mean one of the following? [`State`, `address`, `City`, `Course_type`, `Institute_Name`].;\n'Project [Course_type#2285, Head_Office_Contact#2286L, Institute_Name#2287, City#2523, State#2524, address#2525, explode('branches) AS branche#2569]\n+- Project [Course_type#2285, Head_Office_Contact#2286L, Institute_Name#2287, City#2523, State#2524, address#2525]\n   +- Project [Course_type#2285, Head_Office_Contact#2286L, Institute_Name#2287, branche#2513, branche#2513.City AS City#2523, branche#2513.State AS State#2524, branche#2513.address AS address#2525]\n      +- Project [Course_type#2285, Head_Office_Contact#2286L, Institute_Name#2287, branche#2513]\n         +- Project [Course_type#2285, Head_Office_Contact#2286L, Institute_Name#2287, branches#2288, branche#2513]\n            +- Generate explode(branches#2288), false, [branche#2513]\n               +- Project [Course_type#2285, Head_Office_Contact#2286L, Institute_Name#2287, branches#2288, branche#2481]\n                  +- Generate explode(branches#2288), false, [branche#2481]\n                     +- Project [Course_type#2285, Head_Office_Contact#2286L, Institute_Name#2287, branches#2288, branche#2449]\n                        +- Generate explode(branches#2288), true, [branche#2449]\n                           +- Project [Course_type#2285, Head_Office_Contact#2286L, Institute_Name#2287, branches#2288, branche#2417]\n                              +- Generate explode(branches#2288), false, [branche#2417]\n                                 +- Project [Course_type#2285, Head_Office_Contact#2286L, Institute_Name#2287, branches#2288, branche#2385]\n                                    +- Generate explode(branches#2288), true, [branche#2385]\n                                       +- Project [Course_type#2285, Head_Office_Contact#2286L, Institute_Name#2287, branches#2288, branche#2378]\n                                          +- Generate explode(branches#2288), true, [branche#2378]\n                                             +- Relation [Course_type#2285,Head_Office_Contact#2286L,Institute_Name#2287,branches#2288] json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GhvtkiaC8XRp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}