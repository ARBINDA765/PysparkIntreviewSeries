{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "81153bc1-43d0-4e3c-b28a-04395ecdd3ca",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Sample data\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.window import *\n",
    "data = [(1, 10), (1, 20), (1, 30), (1, 40), \n",
    " (2, 20), (2, 40), (2, 60), (2, 80)]\n",
    "\n",
    "# Create DataFrame\n",
    "df = spark.createDataFrame(data, [\"id\", \"total\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "65e12bfd-0674-4bd4-83f1-35bb23875b1c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Approach-1\n",
    "# Define a window partitioned by 'id' and ordered by 'total'\n",
    "window_spec = Window.partitionBy(\"id\").orderBy(\"total\")\n",
    "\n",
    "# Create a new column 'running_total' which is the cumulative sum of 'total'\n",
    "df = df.withColumn(\"running_total\", sum(\"total\").over(window_spec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "78dd2e6e-91a6-494f-92e4-5b703455322f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+-------------+\n| id|total|running_total|\n+---+-----+-------------+\n|  1|   10|           10|\n|  1|   20|           30|\n|  1|   30|           60|\n|  1|   40|          100|\n|  2|   20|           20|\n|  2|   40|           60|\n|  2|   60|          120|\n|  2|   80|          200|\n+---+-----+-------------+\n\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0c266e7b-4e12-4c8b-adc2-1a837bc71571",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Cummuative Sum in Pyspark",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
