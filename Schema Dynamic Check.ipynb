{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "36cfe2a0-794f-4b8b-960b-1a69d1dc0020",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "####How do i apply the schema from another excel "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4e14ca63-4ea9-4669-b117-8fa4d75e9dc7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.window import *\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Create DataFrame and Insert Data\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "35e39b89-2117-4c4d-9a90-b08e67b91415",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#File uploaded to /FileStore/Youtube_videos_list.txt\n",
    "df=spark.read.format(\"text\")\\\n",
    "    .option(\"header\",\"false\")\\\n",
    "        .load(\"/FileStore/Youtube_videos_list.txt\")\n",
    "\n",
    "columns=[\"Sno\",\"Descirption\",\"Links\"]\n",
    "#df.toDF(*columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7a83c8bc-72bf-4376-92c3-4a3882d0b796",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Schema Comparison\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Define data for the first DataFrame\n",
    "data1 = [\n",
    "    (1, \"John Doe\", \"Engineering\", 100000,100),\n",
    "    (2, \"Jane Smith\", \"Marketing\", 95000,101),\n",
    "    (3, \"Sam Brown\", \"Sales\", 80000,102)\n",
    "]\n",
    "\n",
    "# Define schema for the first DataFrame\n",
    "schema1 = [\"EmployeeID\", \"Name\", \"Department\", \"Salary\",\"Dept ID\"]\n",
    "\n",
    "# Define data for the second DataFrame\n",
    "data2 = [\n",
    "    (1, \"John Doe\", \"Engineering\", 100000, \"Full-time\"),\n",
    "    (2, \"Jane Smith\", \"Marketing\", 95000, \"Full-time\"),\n",
    "    (3, \"Alice Green\", \"HR\", 90000, \"Part-time\")\n",
    "]\n",
    "\n",
    "# Define schema for the second DataFrame\n",
    "schema2 = [\"EmployeeID\", \"Name\", \"Department\", \"Salary\", \"EmploymentType\"]\n",
    "\n",
    "# Create the first DataFrame\n",
    "df1 = spark.createDataFrame(data1, schema=schema1)\n",
    "\n",
    "# Create the second DataFrame\n",
    "df2 = spark.createDataFrame(data2, schema=schema2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cb7b445d-4878-4291-bfaa-ba73faca243e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "####list of the columns Missing in  DF_2  (Present in DF_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c658f9a6-e657-46dd-a924-bc7edde01236",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[5]: ['Dept ID']"
     ]
    }
   ],
   "source": [
    "list(set(df1.columns)-set(df2.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "62160616-a16e-4f13-a4a9-1988b2360e1b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "####list of the columns Missing in  DF_1  (Present in DF_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d0e9d685-8438-45db-8374-77032db896d8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[6]: ['EmploymentType']"
     ]
    }
   ],
   "source": [
    "list(set(df2.columns)-set(df1.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "958dbc90-c12c-4e95-9fe3-c45e25734e40",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "####Ask is to make a one dataframe by union - Dynamic code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "600b2f98-fe6a-4e3e-aa74-13e0d3de0236",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Function to align DataFrames by adding missing columns with null values\n",
    "if df1.columns!=df2.columns:\n",
    "    unique_columns=list(set(df1.columns+df2.columns))\n",
    "    for col in unique_columns:\n",
    "        if col not in df1.columns:\n",
    "            df1=df1.withColumn(col,lit(None))\n",
    "        elif col not in df2.columns:\n",
    "            df2=df2.withColumn(col,lit(None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1111f84b-bdd3-471c-96aa-bc6df34a1072",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['EmployeeID', 'Name', 'Department', 'Salary', 'Dept ID', 'EmploymentType']\n"
     ]
    }
   ],
   "source": [
    "print(df1.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8ed1e83f-cbca-47ec-8003-8321da260187",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['EmployeeID', 'Name', 'Department', 'Salary', 'EmploymentType', 'Dept ID']\n"
     ]
    }
   ],
   "source": [
    "print(df2.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "07dbcc53-0a4d-41b2-b17b-7d89e7026229",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "####Column Names & DATA TYPE CHECK "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fc65af87-0138-4f37-9962-88d37c4e98d1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define data and schema for the master DataFrame\n",
    "master_data = [\n",
    "    (1, \"John Doe\", \"Engineering\", 100000, \"Full-time\"),\n",
    "    (2, \"Jane Smith\", \"Marketing\", 95000, \"Full-time\")\n",
    "]\n",
    "\n",
    "master_schema = StructType([\n",
    "    StructField(\"EmployeeID\", IntegerType(), True),\n",
    "    StructField(\"FullName\", StringType(), True),\n",
    "    StructField(\"Department\", StringType(), True),\n",
    "    StructField(\"Salary\", IntegerType(), True),\n",
    "    StructField(\"EmploymentType\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Create the master DataFrame\n",
    "master_df = spark.createDataFrame(master_data, schema=master_schema)\n",
    "\n",
    "\n",
    "# Define data and schema for another DataFrame with a different schema\n",
    "data = [\n",
    "    (1, \"John Doe\", \"Engineering\", 100000),\n",
    "    (2, \"Jane Smith\", \"Marketing\", 95000),\n",
    "    (3, \"Sam Brown\", \"Sales\", \"80000\")\n",
    "]\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"EmpID\", IntegerType(), True),\n",
    "    StructField(\"Name\", StringType(), True),\n",
    "    StructField(\"Dept\", StringType(), True),\n",
    "    StructField(\"Salary\", StringType(), True)  # Note the different type here\n",
    "])\n",
    "\n",
    "# Create the DataFrame\n",
    "df = spark.createDataFrame(data, schema=schema)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b0bbe12b-7a07-4637-84e5-b65eccdd3c56",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    " #master_df is the master dataframe which we will refer to check the datatype,name \n",
    " #We will allign all the upcoming data according to master_df datatype,name & df is up stream data \n",
    " master_schema = master_df.schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2b56ca38-bd19-41e1-ad8a-590eefa104f0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "####Rename Column Names and Data Types Dynamically\n",
    "def align_with_master(df, master_df):\n",
    "    # Get the schema of the master DataFrame\n",
    "    master_schema = master_df.schema\n",
    "    \n",
    "    # Create a mapping from the master schema\n",
    "    master_columns = {field.name: field.dataType for field in master_schema.fields}\n",
    "    \n",
    "    # Rename columns to match the master DataFrame\n",
    "    for old_name, new_name in zip(df.columns, master_df.columns):\n",
    "        if old_name not in master_columns:\n",
    "            df = df.withColumnRenamed(old_name, new_name)\n",
    "    \n",
    "    # Ensure the columns are in the same order and types as the master DataFrame\n",
    "    for field in master_schema.fields:\n",
    "        if field.name not in df.columns:\n",
    "            # Add missing columns with null values\n",
    "            df = df.withColumn(field.name, lit(None).cast(field.dataType))\n",
    "        else:\n",
    "            # Cast columns to the correct data type\n",
    "            df = df.withColumn(field.name, col(field.name).cast(field.dataType))\n",
    "    \n",
    "    # Select the columns in the same order as the master DataFrame\n",
    "    df = df.select([field.name for field in master_schema.fields])\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "32efd161-217b-4cc8-a8e4-92d9247f12ff",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+-----------+------+\n|EmpID|      Name|       Dept|Salary|\n+-----+----------+-----------+------+\n|    1|  John Doe|Engineering|100000|\n|    2|Jane Smith|  Marketing| 95000|\n|    3| Sam Brown|      Sales| 80000|\n+-----+----------+-----------+------+\n\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Schema Automatically",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
