{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPzkTButZs7xU13zJ0euffg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ARBINDA765/PysparkIntreviewSeries/blob/main/Income_Tax_Returns.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "id": "NZ-UH0Bhk4f9",
        "outputId": "fb474481-442b-4424-917c-46adce879bbc"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x79311c09ae60>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://10e183128c71:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.5.1</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>pyspark-shell</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q http://archive.apache.org/dist/spark/spark-3.5.1/spark-3.5.1-bin-hadoop3.tgz\n",
        "!tar xf spark-3.5.1-bin-hadoop3.tgz\n",
        "!pip install -q findspark\n",
        "import findspark\n",
        "\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.5.1-bin-hadoop3\"\n",
        "\n",
        "findspark.init()\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
        "spark.conf.set(\"spark.sql.repl.eagerEval.enabled\", True) # Property used to format output tables better\n",
        "spark"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DateType\n",
        "from datetime import datetime\n",
        "from pyspark.sql.functions import *\n",
        "\n",
        "# Initialize SparkSession\n",
        "spark = SparkSession.builder.appName(\"Create DataFrame\").getOrCreate()\n",
        "\n",
        "# Define the schema\n",
        "users_data_schema = StructType([\n",
        "    StructField(\"user_id\", IntegerType(), True),\n",
        "    StructField(\"financial_year\", StringType(), True),\n",
        "    StructField(\"return_file_date\", DateType(), True)\n",
        "])\n",
        "\n",
        "# Create the data and convert date strings to datetime.date objects\n",
        "users_data= [\n",
        "    (1, \"FY20\", datetime.strptime(\"2020-05-10\", \"%Y-%m-%d\").date()),\n",
        "    (1, \"FY21\", datetime.strptime(\"2021-10-10\", \"%Y-%m-%d\").date()),\n",
        "    (1, \"FY23\", datetime.strptime(\"2023-08-20\", \"%Y-%m-%d\").date()),\n",
        "    (2, \"FY20\", datetime.strptime(\"2020-05-15\", \"%Y-%m-%d\").date()),\n",
        "    (2, \"FY21\", datetime.strptime(\"2021-09-10\", \"%Y-%m-%d\").date()),\n",
        "    (2, \"FY22\", datetime.strptime(\"2022-08-20\", \"%Y-%m-%d\").date()),\n",
        "    (2, \"FY23\", datetime.strptime(\"2023-10-10\", \"%Y-%m-%d\").date())\n",
        "]\n",
        "\n",
        "# Create DataFrame\n",
        "users_df = spark.createDataFrame(users_data, users_data_schema )\n",
        "\n",
        "# Show DataFrame\n",
        "users_df.show()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Initialize SparkSession\n",
        "spark = SparkSession.builder.appName(\"Create DataFrame\").getOrCreate()\n",
        "\n",
        "# Define the schema\n",
        "income_tax_dates_schema = StructType([\n",
        "    StructField(\"financial_year\", StringType(), True),\n",
        "    StructField(\"file_start_date\", DateType(), True),\n",
        "    StructField(\"file_due_date\", DateType(), True)\n",
        "])\n",
        "\n",
        "# Create the data and convert date strings to datetime.date objects\n",
        "income_tax_dates_data = [\n",
        "    (\"FY20\", datetime.strptime(\"2020-05-01\", \"%Y-%m-%d\").date(), datetime.strptime(\"2020-08-31\", \"%Y-%m-%d\").date()),\n",
        "    (\"FY21\", datetime.strptime(\"2021-06-01\", \"%Y-%m-%d\").date(), datetime.strptime(\"2021-09-30\", \"%Y-%m-%d\").date()),\n",
        "    (\"FY22\", datetime.strptime(\"2022-05-05\", \"%Y-%m-%d\").date(), datetime.strptime(\"2022-08-29\", \"%Y-%m-%d\").date()),\n",
        "    (\"FY23\", datetime.strptime(\"2023-05-05\", \"%Y-%m-%d\").date(), datetime.strptime(\"2023-08-31\", \"%Y-%m-%d\").date())\n",
        "]\n",
        "\n",
        "# Create DataFrame\n",
        "income_tax_dates_df = spark.createDataFrame(income_tax_dates_data, income_tax_dates_schema)\n",
        "\n",
        "# Show DataFrame\n",
        "income_tax_dates_df.show()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZEyRSl1ol8y0",
        "outputId": "ebf1aff4-1f76-439d-c018-aeced226b17b"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+--------------+----------------+\n",
            "|user_id|financial_year|return_file_date|\n",
            "+-------+--------------+----------------+\n",
            "|      1|          FY20|      2020-05-10|\n",
            "|      1|          FY21|      2021-10-10|\n",
            "|      1|          FY23|      2023-08-20|\n",
            "|      2|          FY20|      2020-05-15|\n",
            "|      2|          FY21|      2021-09-10|\n",
            "|      2|          FY22|      2022-08-20|\n",
            "|      2|          FY23|      2023-10-10|\n",
            "+-------+--------------+----------------+\n",
            "\n",
            "+--------------+---------------+-------------+\n",
            "|financial_year|file_start_date|file_due_date|\n",
            "+--------------+---------------+-------------+\n",
            "|          FY20|     2020-05-01|   2020-08-31|\n",
            "|          FY21|     2021-06-01|   2021-09-30|\n",
            "|          FY22|     2022-05-05|   2022-08-29|\n",
            "|          FY23|     2023-05-05|   2023-08-31|\n",
            "+--------------+---------------+-------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#We will do the cross join both the table so assigning a value to both table\n",
        "users_df=users_df.withColumn(\"key\",lit(1))\n",
        "income_tax_dates_df=income_tax_dates_df.withColumn(\"key\",lit(1))\n"
      ],
      "metadata": {
        "id": "-tT-JwUQmbBv"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "lExQvGkAn23p"
      }
    },
    {
      "source": [
        "#Cross join on key columns\n",
        "all_users_year=income_tax_dates_df.alias(\"itd\").join(users_df.alias(\"a\"),on=\"key\",how=\"outer\").drop(\"key\")[[\"a.user_id\",\"itd.financial_year\",\"itd.file_due_date\"]] # Use \"itd\" as the alias for user_id column\n",
        "#Removing duplicates\n",
        "all_users_year=all_users_year.drop_duplicates()\n",
        "#Join with user table to get the output\n",
        "ans_df = all_users_year.join(users_df, on=[\"user_id\", \"financial_year\"], how=\"left\").drop(\"key\")\n",
        "#sort on FY year\n",
        "ans_df = ans_df.orderBy(\"user_id\", \"financial_year\")\n",
        "\n",
        "#filter the ans_df[\"return_file_date\"]>ans_df[\"file_due_date\"])  and isnull(ans_df[\"return_file_date\"]\n",
        "ans_df=ans_df.filter((ans_df[\"return_file_date\"]>ans_df[\"file_due_date\"]) | (isnull(ans_df[\"return_file_date\"])))\n",
        "#Categories the data\n",
        "ans_df=ans_df.withColumn(\"comment\",when(ans_df[\"return_file_date\"]>ans_df[\"file_due_date\"],\"late\").\n",
        "                         when(isnull(ans_df[\"return_file_date\"]),\"missed\").\n",
        "                         otherwise(\"unknown\")\n",
        "                         )\n",
        "ans_df=ans_df.drop(\"return_file_date\",\"file_due_date\")\n",
        "ans_df.show()\n",
        "\n"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OQxoNgq9oaYm",
        "outputId": "6c2fb928-983f-4c76-90c7-a7dddc1693e1"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+--------------+-------+\n",
            "|user_id|financial_year|comment|\n",
            "+-------+--------------+-------+\n",
            "|      1|          FY21|   late|\n",
            "|      1|          FY22| missed|\n",
            "|      2|          FY23|   late|\n",
            "+-------+--------------+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "U5o0TPfgnQfP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}