{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cf03506b-bd14-4ae1-b3dc-fac72bd68414",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "####Explode the columns as value as string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b484f3a3-98bd-461c-b6ff-d7358d718507",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Importing all the required lib\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.window import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "673c4436-02b0-4006-a61f-1ca92c9c7ef3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------+\n| ID|          Name|\n+---+--------------+\n| 11|Arabinda,Shyam|\n| 12|Sunil,Ram,Hari|\n| 13|          Jonh|\n| 14|  Derell,Raghu|\n| 15|          Jonh|\n+---+--------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
    "\n",
    "# Initialize SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Create DataFrame\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Define schema for the DataFrame\n",
    "schema = StructType([\n",
    "    StructField(\"ID\", IntegerType(), True),\n",
    "    StructField(\"Name\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Data\n",
    "data = [\n",
    "    (11, \"Arabinda,Shyam\"),\n",
    "    (12, \"Sunil,Ram,Hari\"),\n",
    "    (13, \"Jonh\"),\n",
    "    (14, \"Derell,Raghu\"),\n",
    "    (15, \"Jonh\")\n",
    "]\n",
    "\n",
    "# Create DataFrame\n",
    "df = spark.createDataFrame(data, schema)\n",
    "\n",
    "# Show DataFrame\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aa24ddc0-adbf-49f4-a2f8-dacfd745fa3f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------+\n|  ID|    Name|\n+----+--------+\n|null|       B|\n|null|    null|\n|null|    null|\n|  11|Arabinda|\n|  12|   Shyam|\n|  13|   Sunil|\n|  14|     Ram|\n|  15|    Hari|\n|  16|    John|\n+----+--------+\n\n"
     ]
    }
   ],
   "source": [
    "df=df.withColumn(\"Name\",explode_outer(split(col(\"Name\"),\",\")))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f2a4580f-7e75-4df7-be18-ba5b29742340",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "####Skip the First n rows in pysaprk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "860a2c8b-ecc8-4483-aae4-43d4554d9ec5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Define schema for the DataFrame\n",
    "schema = StructType([\n",
    "    StructField(\"ID\", IntegerType(), True),\n",
    "    StructField(\"Name\", StringType(), True)\n",
    "])\n",
    "\n",
    "\n",
    "# Data\n",
    "data = [\n",
    "    (None, \"B\"),\n",
    "    (None, None),\n",
    "    (None, None),\n",
    "    (11, \"Arabinda\"),\n",
    "    (12, \"Shyam\"),\n",
    "    (13, \"Sunil\"),\n",
    "    (14, \"Ram\"),\n",
    "    (15, \"Hari\"),\n",
    "    (16, \"John\")\n",
    "]\n",
    "\n",
    "\n",
    "# Create DataFrame for data1\n",
    "df = spark.createDataFrame(data, schema)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "83a0ed9a-49a6-4821-af6a-c165c3106721",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------+\n|  ID|    Name|\n+----+--------+\n|null|       B|\n|null|    null|\n|null|    null|\n|  11|Arabinda|\n|  12|   Shyam|\n|  13|   Sunil|\n|  14|     Ram|\n|  15|    Hari|\n|  16|    John|\n+----+--------+\n\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "71d903e8-34e3-4a64-a6fc-c2567a04e054",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "####Approach-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "131926f7-fd41-43db-a889-38962ab07ad8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+\n| ID|    Name|\n+---+--------+\n| 11|Arabinda|\n| 12|   Shyam|\n| 13|   Sunil|\n| 14|     Ram|\n| 15|    Hari|\n| 16|    John|\n+---+--------+\n\n"
     ]
    }
   ],
   "source": [
    " #df=df.rdd.zipWithIndex().filter(lambda x: x[1] >= 3).map(lambda x:x[0]).toDF())\n",
    "\n",
    "df_1=df.rdd.zipWithIndex() #Index generating\n",
    "df_2=df_1.filter(lambda x: x[1] >= 3) # filter the index to be >=3\n",
    "df_f=df_2.map(lambda x:x[0]).toDF() # map transformation one value to be output \n",
    " #map(lambda x: x[0]).toDF().show()\n",
    " df_f.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8fcbddcc-e64e-4b4a-8014-892208dee211",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "####Approach-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "31081e4d-64d5-4f9d-a8eb-6b2e17ff5623",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+\n| ID|    Name|\n+---+--------+\n| 11|Arabinda|\n| 12|   Shyam|\n| 13|   Sunil|\n| 14|     Ram|\n| 15|    Hari|\n| 16|    John|\n+---+--------+\n\n"
     ]
    }
   ],
   "source": [
    "# Add a row number column\n",
    "df_00 = df.withColumn(\"row_id\", monotonically_increasing_id())\n",
    "\n",
    "windowSpec = Window.orderBy(\"row_id\")\n",
    "df_11 = df_00.withColumn(\"row_num\", row_number().over(windowSpec))\n",
    "df_final=df_11[df_11[\"row_num\"]>=4].drop(\"row_id\",\"row_num\")\n",
    "\n",
    "df_final.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "932fceab-5cf2-4437-b58d-73def53c5760",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------+\n|  ID|    Name|\n+----+--------+\n|null|    null|\n|null|    null|\n|  11|Arabinda|\n|  12|   Shyam|\n|  13|   Sunil|\n|  14|     Ram|\n|  15|    Hari|\n|  16|    John|\n+----+--------+\n\n"
     ]
    }
   ],
   "source": [
    "df.show(#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3da9c6d4-6d70-468e-8791-979879690a5f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Explode columns using PySpark",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
