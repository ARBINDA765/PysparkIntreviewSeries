{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3d9c8dfe-ebd5-48f5-a09f-7392922b20f1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------------+\n| ID|           Name|\n+---+---------------+\n|  1|Sagar-Prajapati|\n|  2|      Alex-John|\n|  3|      John Cena|\n|  4|        Kim Joe|\n+---+---------------+\n\n"
     ]
    }
   ],
   "source": [
    "# importing Lib\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "from pyspark.sql.session import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"testing\").getOrCreate()\n",
    "data = [(1, \"Sagar-Prajapati\"), (2, \"Alex-John\"), (3, \"John Cena\"), (4, \"Kim Joe\")]\n",
    "schema = \"ID int,Name string\"\n",
    "df = spark.createDataFrame(data, schema)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9f97cc2f-6db4-4237-9205-dc166fc36480",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "####Native Approach\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f290d2ee-e964-4aec-a57b-6be915be2339",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------------+------------------+\n| ID|           Name|          New_Name|\n+---+---------------+------------------+\n|  1|Sagar-Prajapati|[Sagar, Prajapati]|\n|  2|      Alex-John|      [Alex, John]|\n|  3|      John Cena|      [John, Cena]|\n|  4|        Kim Joe|        [Kim, Joe]|\n+---+---------------+------------------+\n\n"
     ]
    }
   ],
   "source": [
    "pattern = r\"( )\"\n",
    "replace = \"-\"\n",
    "df_1 = df.withColumn(\n",
    "    \"New_Name\", split(regexp_replace(col(\"Name\"), pattern, replace), \"-\")\n",
    ")\n",
    "df_final = (\n",
    "    df_1.withColumn(\"First_Name\", col(\"New_Name\")[0])\n",
    "    .withColumn(\"Last_Name\", col(\"New_Name\")[1])\n",
    "    .drop(\"New_Name\", \"Name\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "279cfcd4-9fcf-4678-8415-47c8bd5b0514",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "####Dynamic Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d5dc2327-732c-44ff-b8a3-f239b894f2dc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+---------+\n| ID|First_Name|Last_Name|\n+---+----------+---------+\n|  1|     Sagar|Prajapati|\n|  2|      Alex|     John|\n|  3|      John|     Cena|\n|  4|       Kim|      Joe|\n+---+----------+---------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Get the list of columns dynamically\n",
    "new_name_columns = [\n",
    "    \"First_Name\",\n",
    "    \"Last_Name\",\n",
    "]  # Assuming these are the desired column names\n",
    "\n",
    "# For more understanding\n",
    "df_f = df_1\n",
    "\n",
    "# Dynamically extract elements from the New_Name list into separate columns\n",
    "for i, column_name in enumerate(new_name_columns):\n",
    "    df_f = df_f.withColumn(column_name, col(\"New_Name\")[i])\n",
    "\n",
    "# Drop the New_Name column\n",
    "df_f = df_f.drop(\"New_Name\", \"Name\")  # Dropping both \"New_Name\" and \"Name\" columns\n",
    "\n",
    "# Show the resulting DataFrame\n",
    "df_f.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7560b7ae-5a0f-43eb-9ccc-d6b96cd3564c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "How to handle multiple Delimeter in column - Split by multi delimeter",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
