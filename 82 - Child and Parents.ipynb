{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "79eaf53c-ead3-4494-b7e4-ba0029a90aee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+\n|c_id|p_id|\n+----+----+\n| 145| 202|\n| 145| 107|\n| 278| 305|\n| 278| 155|\n| 329| 425|\n| 329| 227|\n| 534| 586|\n| 618| 904|\n+----+----+\n\n+---+---------+------+\n| id|     name|gender|\n+---+---------+------+\n|107|     Days|     F|\n|145| Hawbaker|     M|\n|155|   Hansel|     F|\n|202|Blackston|     M|\n|227|    Criss|     F|\n|278|   Keffer|     M|\n|305|    Canty|     M|\n|329|  Mozingo|     M|\n|425|     Nolf|     M|\n|534|    Waugh|     M|\n|586|     Tong|     M|\n|618|Dimartino|     M|\n|747|    Beane|     M|\n|878|  Chatmon|     F|\n|904|  Hansard|     F|\n+---+---------+------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.window import *\n",
    "\n",
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder.appName(\"PeopleData\").getOrCreate()\n",
    "\n",
    "# Define the schema for the DataFrame\n",
    "schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), True),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"gender\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Manually input the data\n",
    "data = [\n",
    "    (107, \"Days\", \"F\"),\n",
    "    (145, \"Hawbaker\", \"M\"),\n",
    "    (155, \"Hansel\", \"F\"),\n",
    "    (202, \"Blackston\", \"M\"),\n",
    "    (227, \"Criss\", \"F\"),\n",
    "    (278, \"Keffer\", \"M\"),\n",
    "    (305, \"Canty\", \"M\"),\n",
    "    (329, \"Mozingo\", \"M\"),\n",
    "    (425, \"Nolf\", \"M\"),\n",
    "    (534, \"Waugh\", \"M\"),\n",
    "    (586, \"Tong\", \"M\"),\n",
    "    (618, \"Dimartino\", \"M\"),\n",
    "    (747, \"Beane\", \"M\"),\n",
    "    (878, \"Chatmon\", \"F\"),\n",
    "    (904, \"Hansard\", \"F\"),\n",
    "]\n",
    "\n",
    "# Create the DataFrame\n",
    "people = spark.createDataFrame(data, schema=schema)\n",
    "\n",
    "\n",
    "\n",
    "# Define the schema for the relations DataFrame\n",
    "schema_relations = StructType([\n",
    "    StructField(\"c_id\", IntegerType(), True),\n",
    "    StructField(\"p_id\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "# Manually input the data\n",
    "data_relations = [\n",
    "    (145, 202),\n",
    "    (145, 107),\n",
    "    (278, 305),\n",
    "    (278, 155),\n",
    "    (329, 425),\n",
    "    (329, 227),\n",
    "    (534, 586),\n",
    "    (618, 904)\n",
    "]\n",
    "\n",
    "# Create the relations DataFrame\n",
    "relations = spark.createDataFrame(data_relations, schema=schema_relations)\n",
    "\n",
    "# Show the DataFrame\n",
    "relations.show()\n",
    "\n",
    "\n",
    "# Show the DataFrame\n",
    "people.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fd8a3a4b-6257-4743-afeb-0fab1f5a6daa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+-----------+\n|child_name|Mother_Name|Father_Name|\n+----------+-----------+-----------+\n| Dimartino|       null|    Hansard|\n|  Hawbaker|  Blackston|       Days|\n|    Keffer|      Canty|     Hansel|\n|   Mozingo|       Nolf|      Criss|\n|     Waugh|       Tong|       null|\n+----------+-----------+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "ans_df=relations.join(people, relations.p_id == people.id, how=\"left\") \\\n",
    "    .select(\n",
    "        relations.c_id.alias(\"child_id\"),\n",
    "        relations.p_id.alias(\"parent_id\"),\n",
    "        when(col(\"gender\") == \"M\", col(\"name\")).otherwise(None).alias(\"Mother_Name\"),\n",
    "         when(col(\"gender\") == \"F\", col(\"name\")).otherwise(None).alias(\"Father_Name\")\n",
    "    )\n",
    "# Alias the DataFrames\n",
    "ans_df_alias = ans_df.alias(\"ans\")\n",
    "people_alias = people.alias(\"ppl\")\n",
    "\n",
    "# Perform groupBy, aggregation, and join\n",
    "final_df = (\n",
    "    ans_df_alias.groupBy(\"child_id\")\n",
    "    .agg(\n",
    "        max(\"Mother_Name\").alias(\"Mother_Name\"),\n",
    "        max(\"Father_Name\").alias(\"Father_Name\")\n",
    "    )\n",
    "    .join(people_alias, col(\"child_id\") == col(\"ppl.id\"), how=\"inner\")\n",
    "    .select(\n",
    "        col(\"ppl.name\").alias(\"child_name\"),\n",
    "        col(\"Mother_Name\"),\n",
    "        col(\"Father_Name\")\n",
    "    ).orderBy(col(\"child_name\").asc())\n",
    ")\n",
    "\n",
    "# Show the result\n",
    "final_df.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "94f47499-3cbe-4331-89c1-20230faf18c4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+-----------+\n|child_name|Mother_Name|Father_Name|\n+----------+-----------+-----------+\n| Dimartino|       null|    Hansard|\n|  Hawbaker|  Blackston|       Days|\n|    Keffer|      Canty|     Hansel|\n|   Mozingo|       Nolf|      Criss|\n|     Waugh|       Tong|       null|\n+----------+-----------+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "# Alias the DataFrames\n",
    "ans_df_alias = ans_df.alias(\"ans\")\n",
    "people_alias = people.alias(\"ppl\")\n",
    "\n",
    "# group by chil_id & then join the chil_id with ppl_id to get the child_name \n",
    "final_df = (\n",
    "    ans_df_alias.groupBy(\"child_id\")\n",
    "    .agg(\n",
    "        max(\"Mother_Name\").alias(\"Mother_Name\"),\n",
    "        max(\"Father_Name\").alias(\"Father_Name\")\n",
    "    )\n",
    "    .join(people_alias, col(\"child_id\") == col(\"ppl.id\"), how=\"inner\")\n",
    "    .select(\n",
    "        col(\"ppl.name\").alias(\"child_name\"),\n",
    "        col(\"Mother_Name\"),\n",
    "        col(\"Father_Name\")\n",
    "    ).orderBy(col(\"child_name\").asc())\n",
    ")\n",
    "\n",
    "# Show the result\n",
    "final_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6b207981-82f6-4528-8b19-ea94f77ba6e6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "82 - Child and Parents",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
